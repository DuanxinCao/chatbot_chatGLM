{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77019894",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "import torch\n",
    "\n",
    "def torch_gc():\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"clear cache\")\n",
    "        # with torch.cuda.device(DEVICE):\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    elif torch.backends.mps.is_available():\n",
    "        try:\n",
    "            from torch.mps import empty_cache\n",
    "            empty_cache()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"如果您使用的是 macOS 建议将 pytorch 版本升级至 2.0.0 或更高版本，以支持及时清理 torch 产生的内存占用。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78c6e684",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import re\n",
    "from typing import List\n",
    "SENTENCE_SIZE=100\n",
    "class ChineseTextSplitter(CharacterTextSplitter):\n",
    "    def __init__(self, pdf: bool = False, sentence_size: int = SENTENCE_SIZE, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pdf = pdf\n",
    "        self.sentence_size = sentence_size\n",
    "\n",
    "    def split_text1(self, text: str) -> List[str]:\n",
    "        if self.pdf:\n",
    "            text = re.sub(r\"\\n{3,}\", \"\\n\", text)\n",
    "            text = re.sub('\\s', ' ', text)\n",
    "            text = text.replace(\"\\n\\n\", \"\")\n",
    "        sent_sep_pattern = re.compile('([﹒﹔﹖﹗．。！？][\"’”」』]{0,2}|(?=[\"‘“「『]{1,2}|$))')  # del ：；\n",
    "        sent_list = []\n",
    "        for ele in sent_sep_pattern.split(text):\n",
    "            if sent_sep_pattern.match(ele) and sent_list:\n",
    "                sent_list[-1] += ele\n",
    "            elif ele:\n",
    "                sent_list.append(ele)\n",
    "        return sent_list\n",
    "\n",
    "    def split_text(self, text: str) -> List[str]:   ##此处需要进一步优化逻辑\n",
    "        if self.pdf:\n",
    "            text = re.sub(r\"\\n{3,}\", r\"\\n\", text)\n",
    "            text = re.sub('\\s', \" \", text)\n",
    "            text = re.sub(\"\\n\\n\", \"\", text)\n",
    "\n",
    "        text = re.sub(r'([;；.!?。！？\\?])([^”’])', r\"\\1\\n\\2\", text)  # 单字符断句符\n",
    "        text = re.sub(r'(\\.{6})([^\"’”」』])', r\"\\1\\n\\2\", text)  # 英文省略号\n",
    "        text = re.sub(r'(\\…{2})([^\"’”」』])', r\"\\1\\n\\2\", text)  # 中文省略号\n",
    "        text = re.sub(r'([;；!?。！？\\?][\"’”」』]{0,2})([^;；!?，。！？\\?])', r'\\1\\n\\2', text)\n",
    "        # 如果双引号前有终止符，那么双引号才是句子的终点，把分句符\\n放到双引号后，注意前面的几句都小心保留了双引号\n",
    "        text = text.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "        # 很多规则中会考虑分号;，但是这里我把它忽略不计，破折号、英文双引号等同样忽略，需要的再做些简单调整即可。\n",
    "        ls = [i for i in text.split(\"\\n\") if i]\n",
    "        for ele in ls:\n",
    "            if len(ele) > self.sentence_size:\n",
    "                ele1 = re.sub(r'([,，.][\"’”」』]{0,2})([^,，.])', r'\\1\\n\\2', ele)\n",
    "                ele1_ls = ele1.split(\"\\n\")\n",
    "                for ele_ele1 in ele1_ls:\n",
    "                    if len(ele_ele1) > self.sentence_size:\n",
    "                        ele_ele2 = re.sub(r'([\\n]{1,}| {2,}[\"’”」』]{0,2})([^\\s])', r'\\1\\n\\2', ele_ele1)\n",
    "                        ele2_ls = ele_ele2.split(\"\\n\")\n",
    "                        for ele_ele2 in ele2_ls:\n",
    "                            if len(ele_ele2) > self.sentence_size:\n",
    "                                ele_ele3 = re.sub('( [\"’”」』]{0,2})([^ ])', r'\\1\\n\\2', ele_ele2)\n",
    "                                ele2_id = ele2_ls.index(ele_ele2)\n",
    "                                ele2_ls = ele2_ls[:ele2_id] + [i for i in ele_ele3.split(\"\\n\") if i] + ele2_ls[\n",
    "                                                                                                       ele2_id + 1:]\n",
    "                        ele_id = ele1_ls.index(ele_ele1)\n",
    "                        ele1_ls = ele1_ls[:ele_id] + [i for i in ele2_ls if i] + ele1_ls[ele_id + 1:]\n",
    "\n",
    "                id = ls.index(ele)\n",
    "                ls = ls[:id] + [i for i in ele1_ls if i] + ls[id + 1:]\n",
    "        return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader, TextLoader\n",
    "\n",
    "filepath = \"/root/caoduanxin/docs/tairQA_cn.txt\"\n",
    "\n",
    "loader = TextLoader(filepath,autodetect_encoding=True)\n",
    "textsplitter = ChineseTextSplitter(pdf=False, sentence_size=100)\n",
    "print(f'textsplitter:{textsplitter}')\n",
    "\n",
    "docs = loader.load_and_split(textsplitter)\n",
    "print(docs)\n",
    "for doc in docs:\n",
    "    print(f'解析后的文本：{doc}')\n",
    "\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredFileLoader, TextLoader\n",
    "\n",
    "filedir = \"/root/caoduanxin/docs/tair_doc\"\n",
    "docs=[]\n",
    "for root,dirs,files in os.walk(filedir):\n",
    "    for file in files:\n",
    "        filepath=os.path.join(root,file)\n",
    "        loader = TextLoader(filepath,autodetect_encoding=True)\n",
    "        textsplitter = ChineseTextSplitter(pdf=False, sentence_size=100)\n",
    "        docs_tmp = loader.load_and_split(textsplitter)\n",
    "        docs.append(docs_tmp)\n",
    "        \n",
    "print(docs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import torch\n",
    "EMBEDDING_DEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name='GanymedeNil/text2vec-large-chinese',\n",
    "                                                model_kwargs={'device': EMBEDDING_DEVICE})\n",
    "\n",
    "torch_gc()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Tair\n",
    "keys=[]\n",
    "for doc_index in range(len(docs)):\n",
    "    keys.append(f'doc{doc_index}')\n",
    "for key in keys:\n",
    "    print(key)\n",
    "vector_store = Tair.from_documents(docs,embeddings,tair_url=\"redis://120.27.213.45:6380\",keys=keys)  # docs 为Document列表"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pypinyin import lazy_pinyin\n",
    "import datetime\n",
    "\n",
    "VS_ROOT_PATH = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))), \"vector_store\")\n",
    "print(f'{VS_ROOT_PATH}')\n",
    "\n",
    "file = os.path.split(filepath)[-1]\n",
    "print(file)\n",
    "\n",
    "vs_path = os.path.join(VS_ROOT_PATH, f\"\"\"{\"\".join(lazy_pinyin(os.path.splitext(file)[0]))}_FAISS_{datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")}\"\"\")\n",
    "print(f'{vs_path}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query=\"Tair是什么？\"\n",
    "context = vector_store.similarity_search(query, k=1)\n",
    "context = \"\\n\".join([doc.page_content for doc in context])\n",
    "print(context)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"已知信息：\n",
    "{context} \n",
    "\n",
    "根据上述已知信息，简洁和专业的来回答用户的问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题” 或 “没有提供足够的相关信息”，不允许在答案中添加编造成分，答案请使用中文。 问题是：{question}\"\"\"\n",
    "prompt = PROMPT_TEMPLATE.replace(\"{question}\", query).replace(\"{context}\", context)\n",
    "print(f'prompt:{prompt}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"/root/ChatGLM-6B/THUDM/chatglm-6b-int4-qe\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()\n",
    "response, history = model.chat(tokenizer, prompt, history=[])\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "response, history = model.chat(tokenizer, \"vector是什么\", history=[])\n",
    "print(response)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tair\n",
    "client = tair.Tair(host=\"120.27.213.45\", port=6380)\n",
    "client.set(\"vector\",\"vector是向量数据库\")\n",
    "result = client.get(\"vector\")\n",
    "result=str(result, encoding = \"utf-8\")\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tairvector_text=[]\n",
    "tairvector_text.append(result)\n",
    "# tairvector_text.append(\"vector是向量数据库\")\n",
    "\n",
    "\n",
    "tairvector_meta=[{\"source\":\"tair\"}]\n",
    "\n",
    "key_tairvector=[]\n",
    "key_tairvector.append(f'tairvector0')\n",
    "Tair.from_texts(tairvector_text,embeddings,tairvector_meta,\"langchain\",\"content\",\"metadata\",tair_url=\"redis://120.27.213.45:6380\",keys=key_tairvector)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "query=\"vector是什么？\"\n",
    "context = vector_store.similarity_search(query, k=1)\n",
    "context = \"\\n\".join([doc.page_content for doc in context])\n",
    "\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"已知信息：\n",
    "{context} \n",
    "\n",
    "根据上述已知信息，简洁和专业的来回答用户的问题。如果无法从中得到答案，请说 “根据已知信息无法回答该问题” 或 “没有提供足够的相关信息”，不允许在答案中添加编造成分，答案请使用中文。 问题是：{question}\"\"\"\n",
    "prompt = PROMPT_TEMPLATE.replace(\"{question}\", query).replace(\"{context}\", context)\n",
    "print(f'prompt:{prompt}')\n",
    "\n",
    "response, history = model.chat(tokenizer, prompt, history=[])\n",
    "print(response)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}